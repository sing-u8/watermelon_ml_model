"""
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ML í”„ë¡œì íŠ¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

2ë‹¨ê³„ì˜ ëª¨ë“  ê¸°ëŠ¥ì´ ì œëŒ€ë¡œ ì—°ë™ë˜ì–´ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data.dataset import (
    WatermelonDataset, 
    get_augmented_transforms,
    create_stratified_split,
    create_dataloaders,
    setup_cross_validation
)
import torch

def test_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¯ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ML í”„ë¡œì íŠ¸ - 2ë‹¨ê³„ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # 1. ë°ì´í„°ì…‹ ìƒì„± (ìƒˆë¡œìš´ ê²½ë¡œ)
    print("\n1ï¸âƒ£ ë°ì´í„°ì…‹ ìƒì„±...")
    dataset = WatermelonDataset(root_dir="data/features/melspectrogram_data")
    print(f"   âœ… ì´ {len(dataset)}ê°œ ì´ë¯¸ì§€, {len(set([info['sample_id'] for info in dataset.sample_info]))}ê°œ ìˆ˜ë°• ìƒ˜í”Œ")
    
    # 2. ë°ì´í„° ë¶„í• 
    print("\n2ï¸âƒ£ ë°ì´í„° ë¶„í• ...")
    train_indices, val_indices, test_indices = create_stratified_split(dataset)
    print(f"   âœ… ë¶„í•  ì™„ë£Œ: í›ˆë ¨ {len(train_indices)}, ê²€ì¦ {len(val_indices)}, í…ŒìŠ¤íŠ¸ {len(test_indices)}")
    
    # 3. ì¦ê°• ë³€í™˜ ì¤€ë¹„
    print("\n3ï¸âƒ£ ë°ì´í„° ì¦ê°• ë³€í™˜ ì¤€ë¹„...")
    transforms = get_augmented_transforms('medium')
    print(f"   âœ… í›ˆë ¨ìš©/ê²€ì¦ìš© ë³€í™˜ íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
    
    # 4. DataLoader ìƒì„±
    print("\n4ï¸âƒ£ DataLoader ìƒì„±...")
    dataloaders = create_dataloaders(
        dataset=dataset,
        train_indices=train_indices,
        val_indices=val_indices,
        test_indices=test_indices,
        batch_size=4,
        num_workers=1,
        train_transforms=transforms['train'],
        val_transforms=transforms['val']
    )
    print(f"   âœ… {len(dataloaders)}ê°œ DataLoader ìƒì„± ì™„ë£Œ")
    
    # 5. ë°°ì¹˜ ë°ì´í„° ê²€ì¦
    print("\n5ï¸âƒ£ ë°°ì¹˜ ë°ì´í„° ê²€ì¦...")
    for split_name, dataloader in dataloaders.items():
        batch_images, batch_labels = next(iter(dataloader))
        print(f"   âœ… {split_name}: {batch_images.shape} ì´ë¯¸ì§€, {batch_labels.shape} ë¼ë²¨")
        
        # ë°ì´í„° íƒ€ì… ë° ë²”ìœ„ í™•ì¸
        assert batch_images.dtype == torch.float32, f"{split_name} ì´ë¯¸ì§€ íƒ€ì… ì˜¤ë¥˜"
        assert batch_labels.dtype == torch.float32, f"{split_name} ë¼ë²¨ íƒ€ì… ì˜¤ë¥˜"
        assert batch_images.min() >= -5.0 and batch_images.max() <= 5.0, f"{split_name} ì´ë¯¸ì§€ ë²”ìœ„ ì´ìƒ"
    
    # 6. K-Fold ê²€ì¦
    print("\n6ï¸âƒ£ K-Fold êµì°¨ ê²€ì¦...")
    fold_splits = setup_cross_validation(dataset, k_folds=3)
    print(f"   âœ… {len(fold_splits)}ê°œ í´ë“œ ìƒì„± ì™„ë£Œ")
    
    # 7. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì²´í¬
    print("\n7ï¸âƒ£ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì²´í¬...")
    train_loader = dataloaders['train']
    batch_count = 0
    for batch_images, batch_labels in train_loader:
        batch_count += 1
        if batch_count >= 3:  # ì²˜ìŒ 3ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
            break
    print(f"   âœ… {batch_count}ê°œ ë°°ì¹˜ ì—°ì† ë¡œë”© ì„±ê³µ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ)")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    print("\nğŸ“Š ìš”ì•½:")
    print(f"   â€¢ ë°ì´í„°ì…‹: {len(dataset)}ê°œ ì´ë¯¸ì§€")
    print(f"   â€¢ ë¶„í• : {len(train_indices)}/{len(val_indices)}/{len(test_indices)}")
    print(f"   â€¢ DataLoader: ë°°ì¹˜í¬ê¸° 4, {len(dataloaders)}ê°œ")
    print(f"   â€¢ êµì°¨ ê²€ì¦: {len(fold_splits)}-fold")
    print(f"   â€¢ ì¦ê°•: Medium ê°•ë„ ì ìš©")
    
    return True

if __name__ == "__main__":
    try:
        test_full_pipeline()
        print("\nâœ… 2ë‹¨ê³„ ëª¨ë“  ì‘ì—… ì™„ë£Œ! 3ë‹¨ê³„ ëª¨ë¸ ì„¤ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc() 