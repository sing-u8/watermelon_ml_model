"""
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•œ í‰ê°€ ë©”íŠ¸ë¦­ ë° ë¶„ì„ ë„êµ¬

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. íšŒê·€ í‰ê°€ ë©”íŠ¸ë¦­ (RMSE, MAE, RÂ², MAPE, ìƒê´€ê³„ìˆ˜)
2. ë‹¹ë„ ë²”ìœ„ë³„ ì •í™•ë„ í‰ê°€
3. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„
4. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë„êµ¬
5. ìƒì„¸í•œ ì˜¤ì°¨ ë¶„ì„

Author: AI Assistant
Date: 2024
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from scipy import stats
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from ..utils.font_utils import setup_korean_font

# í•œê¸€ í°íŠ¸ ì„¤ì •
setup_korean_font()
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader


class RegressionMetrics:
    """íšŒê·€ ëª¨ë¸ì„ ìœ„í•œ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def compute_basic_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’
            
        Returns:
            Dict: ê³„ì‚°ëœ ë©”íŠ¸ë¦­ë“¤
        """
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # MAPE (Mean Absolute Percentage Error)
        # 0 ê°’ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‘ì€ ê°’ ì¶”ê°€
        epsilon = 1e-8
        mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100
        
        # ìƒê´€ê³„ìˆ˜
        pearson_corr, pearson_p = stats.pearsonr(y_true, y_pred)
        spearman_corr, spearman_p = stats.spearmanr(y_true, y_pred)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­
        # Mean Bias Error (MBE)
        mbe = np.mean(y_pred - y_true)
        
        # Normalized RMSE
        nrmse = rmse / (np.max(y_true) - np.min(y_true)) * 100
        
        # ê²°ì •ê³„ìˆ˜ì˜ ì¡°ì •ëœ ë²„ì „ (ìƒ˜í”Œ ìˆ˜ì™€ ë³€ìˆ˜ ìˆ˜ ê³ ë ¤)
        n = len(y_true)
        p = 1  # íšŒê·€ì—ì„œ ë…ë¦½ë³€ìˆ˜ëŠ” 1ê°œ (ì˜ˆì¸¡ê°’)
        adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if n > p + 1 else r2
        
        return {
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'adjusted_r2': adjusted_r2,
            'mape': mape,
            'mbe': mbe,
            'nrmse': nrmse,
            'pearson_corr': pearson_corr,
            'pearson_p_value': pearson_p,
            'spearman_corr': spearman_corr,
            'spearman_p_value': spearman_p
        }
    
    @staticmethod
    def compute_brix_accuracy(y_true: np.ndarray, 
                             y_pred: np.ndarray,
                             tolerances: List[float] = [0.5, 1.0, 1.5, 2.0]) -> Dict[str, float]:
        """
        ë‹¹ë„ ë²”ìœ„ë³„ ì •í™•ë„ ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œ Brix ê°’
            y_pred: ì˜ˆì¸¡ Brix ê°’
            tolerances: í—ˆìš© ì˜¤ì°¨ ë²”ìœ„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ê° í—ˆìš© ì˜¤ì°¨ë³„ ì •í™•ë„
        """
        accuracies = {}
        
        for tolerance in tolerances:
            # í—ˆìš© ì˜¤ì°¨ ë‚´ì— ìˆëŠ” ì˜ˆì¸¡ì˜ ë¹„ìœ¨
            correct_predictions = np.abs(y_true - y_pred) <= tolerance
            accuracy = np.mean(correct_predictions) * 100
            accuracies[f'accuracy_within_{tolerance}_brix'] = accuracy
            
        return accuracies
    
    @staticmethod
    def compute_class_based_metrics(y_true: np.ndarray, 
                                   y_pred: np.ndarray,
                                   class_boundaries: List[float] = [9.0, 10.5, 12.0]) -> Dict[str, Any]:
        """
        ë‹¹ë„ êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
        
        Args:
            y_true: ì‹¤ì œ Brix ê°’
            y_pred: ì˜ˆì¸¡ Brix ê°’
            class_boundaries: êµ¬ê°„ ê²½ê³„ê°’ë“¤
            
        Returns:
            Dict: êµ¬ê°„ë³„ ë©”íŠ¸ë¦­
        """
        # êµ¬ê°„ ìƒì„± (Low, Medium, High)
        class_names = ['Low', 'Medium', 'High', 'Very_High']
        boundaries = [-np.inf] + class_boundaries + [np.inf]
        
        # ì‹¤ì œ ê°’ì˜ êµ¬ê°„ ë¶„ë¥˜
        true_classes = np.digitize(y_true, class_boundaries)
        pred_classes = np.digitize(y_pred, class_boundaries)
        
        class_metrics = {}
        
        for i, class_name in enumerate(class_names):
            # í•´ë‹¹ êµ¬ê°„ì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤ ì°¾ê¸°
            class_mask = (true_classes == i)
            
            if np.sum(class_mask) > 0:
                class_true = y_true[class_mask]
                class_pred = y_pred[class_mask]
                
                # í•´ë‹¹ êµ¬ê°„ì˜ ë©”íŠ¸ë¦­ ê³„ì‚°
                class_metrics[f'{class_name}_count'] = np.sum(class_mask)
                class_metrics[f'{class_name}_rmse'] = np.sqrt(mean_squared_error(class_true, class_pred))
                class_metrics[f'{class_name}_mae'] = mean_absolute_error(class_true, class_pred)
                
                if len(class_true) > 1:
                    class_metrics[f'{class_name}_r2'] = r2_score(class_true, class_pred)
                    corr, _ = stats.pearsonr(class_true, class_pred)
                    class_metrics[f'{class_name}_correlation'] = corr
                else:
                    class_metrics[f'{class_name}_r2'] = 0.0
                    class_metrics[f'{class_name}_correlation'] = 0.0
                    
                # êµ¬ê°„ ë‚´ í‰ê·  ì˜¤ì°¨
                class_metrics[f'{class_name}_mean_error'] = np.mean(class_pred - class_true)
                class_metrics[f'{class_name}_std_error'] = np.std(class_pred - class_true)
        
        # êµ¬ê°„ë³„ ë¶„ë¥˜ ì •í™•ë„ (ì˜ˆì¸¡ê°’ì´ ì˜¬ë°”ë¥¸ êµ¬ê°„ì— ì†í•˜ëŠ”ì§€)
        class_accuracy = np.mean(true_classes == pred_classes) * 100
        class_metrics['class_accuracy'] = class_accuracy
        
        return class_metrics


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ ë° ë¶„ì„ì„ ìœ„í•œ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, model: nn.Module, device: torch.device = None):
        """
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        """
        self.model = model
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
    def predict(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """
        ë°ì´í„° ë¡œë”ì—ì„œ ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            data_loader: í‰ê°€í•  ë°ì´í„° ë¡œë”
            
        Returns:
            Tuple: (ì˜ˆì¸¡ê°’, ì‹¤ì œê°’)
        """
        self.model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for inputs, targets in data_loader:
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                outputs = self.model(inputs)
                
                all_predictions.extend(outputs.squeeze().cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
        
        return np.array(all_predictions), np.array(all_targets)
    
    def evaluate_comprehensive(self, 
                             data_loader: DataLoader,
                             dataset_name: str = "Test") -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€
        
        Args:
            data_loader: í‰ê°€í•  ë°ì´í„° ë¡œë”
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (ë¡œê·¸ìš©)
            
        Returns:
            Dict: ëª¨ë“  í‰ê°€ ê²°ê³¼
        """
        print(f"\n{'='*50}")
        print(f"{dataset_name} ë°ì´í„° ì¢…í•© í‰ê°€")
        print(f"{'='*50}")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred, y_true = self.predict(data_loader)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        basic_metrics = RegressionMetrics.compute_basic_metrics(y_true, y_pred)
        
        # ë‹¹ë„ ì •í™•ë„ ê³„ì‚°
        brix_accuracies = RegressionMetrics.compute_brix_accuracy(y_true, y_pred)
        
        # êµ¬ê°„ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        class_metrics = RegressionMetrics.compute_class_based_metrics(y_true, y_pred)
        
        # ê²°ê³¼ í†µí•©
        results = {
            'dataset_name': dataset_name,
            'sample_count': len(y_true),
            'predictions': y_pred,
            'targets': y_true,
            **basic_metrics,
            **brix_accuracies,
            **class_metrics
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_evaluation_results(results)
        
        return results
    
    def _print_evaluation_results(self, results: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ“Š ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­:")
        print(f"  â€¢ RMSE: {results['rmse']:.4f}")
        print(f"  â€¢ MAE: {results['mae']:.4f}")
        print(f"  â€¢ RÂ²: {results['r2']:.4f}")
        print(f"  â€¢ ì¡°ì •ëœ RÂ²: {results['adjusted_r2']:.4f}")
        print(f"  â€¢ MAPE: {results['mape']:.2f}%")
        print(f"  â€¢ NRMSE: {results['nrmse']:.2f}%")
        print(f"  â€¢ í‰ê·  í¸í–¥ ì˜¤ì°¨: {results['mbe']:.4f}")
        
        print(f"\nğŸ¯ ë‹¹ë„ ë²”ìœ„ë³„ ì •í™•ë„:")
        for key, value in results.items():
            if key.startswith('accuracy_within'):
                tolerance = key.split('_')[2]
                print(f"  â€¢ Â±{tolerance} Brix ì´ë‚´: {value:.1f}%")
        
        print(f"\nğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„:")
        print(f"  â€¢ Pearson ìƒê´€ê³„ìˆ˜: {results['pearson_corr']:.4f} (p={results['pearson_p_value']:.4f})")
        print(f"  â€¢ Spearman ìƒê´€ê³„ìˆ˜: {results['spearman_corr']:.4f} (p={results['spearman_p_value']:.4f})")
        
        print(f"\nğŸ·ï¸ ë‹¹ë„ êµ¬ê°„ë³„ ì„±ëŠ¥:")
        class_names = ['Low', 'Medium', 'High', 'Very_High']
        for class_name in class_names:
            count_key = f'{class_name}_count'
            if count_key in results and results[count_key] > 0:
                rmse = results[f'{class_name}_rmse']
                mae = results[f'{class_name}_mae']
                r2 = results[f'{class_name}_r2']
                count = results[count_key]
                print(f"  â€¢ {class_name} êµ¬ê°„ (n={count}): RMSE={rmse:.3f}, MAE={mae:.3f}, RÂ²={r2:.3f}")
        
        print(f"  â€¢ êµ¬ê°„ ë¶„ë¥˜ ì •í™•ë„: {results['class_accuracy']:.1f}%")


class VisualizationTools:
    """í‰ê°€ ê²°ê³¼ ì‹œê°í™” ë„êµ¬"""
    
    @staticmethod
    def plot_prediction_vs_actual(y_true: np.ndarray, 
                                  y_pred: np.ndarray,
                                  title: str = "ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’",
                                  save_path: Optional[str] = None,
                                  show_metrics: bool = True):
        """ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ í”Œë¡¯"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ì‚°ì ë„
        ax1.scatter(y_true, y_pred, alpha=0.6, color='blue', s=50)
        
        # ì™„ë²½í•œ ì˜ˆì¸¡ì„  (y=x)
        min_val = min(np.min(y_true), np.min(y_pred))
        max_val = max(np.max(y_true), np.max(y_pred))
        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='ì™„ë²½í•œ ì˜ˆì¸¡')
        
        # íšŒê·€ì„  ì¶”ê°€
        z = np.polyfit(y_true, y_pred, 1)
        p = np.poly1d(z)
        ax1.plot(y_true, p(y_true), 'g-', alpha=0.8, label=f'íšŒê·€ì„  (ê¸°ìš¸ê¸°: {z[0]:.2f})')
        
        ax1.set_xlabel('ì‹¤ì œ Brix ê°’')
        ax1.set_ylabel('ì˜ˆì¸¡ Brix ê°’')
        ax1.set_title(f'{title} - ì‚°ì ë„')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ë©”íŠ¸ë¦­ í…ìŠ¤íŠ¸ ì¶”ê°€
        if show_metrics:
            metrics = RegressionMetrics.compute_basic_metrics(y_true, y_pred)
            textstr = f"RMSE: {metrics['rmse']:.3f}\nMAE: {metrics['mae']:.3f}\nRÂ²: {metrics['r2']:.3f}"
            props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
            ax1.text(0.05, 0.95, textstr, transform=ax1.transAxes, fontsize=10,
                    verticalalignment='top', bbox=props)
        
        # ì˜¤ì°¨ íˆìŠ¤í† ê·¸ë¨
        errors = y_pred - y_true
        ax2.hist(errors, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(np.mean(errors), color='red', linestyle='--', 
                   label=f'í‰ê·  ì˜¤ì°¨: {np.mean(errors):.3f}')
        ax2.axvline(0, color='green', linestyle='-', alpha=0.8, label='ì™„ë²½í•œ ì˜ˆì¸¡ (ì˜¤ì°¨=0)')
        
        ax2.set_xlabel('ì˜ˆì¸¡ ì˜¤ì°¨ (ì˜ˆì¸¡ê°’ - ì‹¤ì œê°’)')
        ax2.set_ylabel('ë¹ˆë„')
        ax2.set_title('ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ì‹œê°í™” ì €ì¥: {save_path}")
        
        plt.show()
    
    @staticmethod
    def plot_residual_analysis(y_true: np.ndarray, 
                              y_pred: np.ndarray,
                              save_path: Optional[str] = None):
        """ì”ì°¨ ë¶„ì„ í”Œë¡¯"""
        
        residuals = y_pred - y_true
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ì”ì°¨ vs ì˜ˆì¸¡ê°’
        axes[0, 0].scatter(y_pred, residuals, alpha=0.6)
        axes[0, 0].axhline(y=0, color='red', linestyle='--')
        axes[0, 0].set_xlabel('ì˜ˆì¸¡ê°’')
        axes[0, 0].set_ylabel('ì”ì°¨')
        axes[0, 0].set_title('ì”ì°¨ vs ì˜ˆì¸¡ê°’')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ì”ì°¨ vs ì‹¤ì œê°’
        axes[0, 1].scatter(y_true, residuals, alpha=0.6)
        axes[0, 1].axhline(y=0, color='red', linestyle='--')
        axes[0, 1].set_xlabel('ì‹¤ì œê°’')
        axes[0, 1].set_ylabel('ì”ì°¨')
        axes[0, 1].set_title('ì”ì°¨ vs ì‹¤ì œê°’')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
        axes[1, 0].hist(residuals, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[1, 0].axvline(np.mean(residuals), color='red', linestyle='--', 
                          label=f'í‰ê· : {np.mean(residuals):.3f}')
        axes[1, 0].set_xlabel('ì”ì°¨')
        axes[1, 0].set_ylabel('ë¹ˆë„')
        axes[1, 0].set_title('ì”ì°¨ ë¶„í¬')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. Q-Q plot (ì •ê·œì„± ê²€ì •)
        stats.probplot(residuals, dist="norm", plot=axes[1, 1])
        axes[1, 1].set_title('Q-Q Plot (ì”ì°¨ ì •ê·œì„± ê²€ì •)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ì”ì°¨ ë¶„ì„ ì €ì¥: {save_path}")
        
        plt.show()
    
    @staticmethod
    def plot_class_performance(results: Dict[str, Any],
                              save_path: Optional[str] = None):
        """êµ¬ê°„ë³„ ì„±ëŠ¥ ë¹„êµ í”Œë¡¯"""
        
        class_names = ['Low', 'Medium', 'High', 'Very_High']
        class_labels = ['ë‚®ìŒ (< 9.0)', 'ë³´í†µ (9.0-10.5)', 'ë†’ìŒ (10.5-12.0)', 'ë§¤ìš° ë†’ìŒ (â‰¥ 12.0)']
        
        # ë°ì´í„° ì¤€ë¹„
        counts = []
        rmse_values = []
        mae_values = []
        r2_values = []
        
        for class_name in class_names:
            count_key = f'{class_name}_count'
            if count_key in results and results[count_key] > 0:
                counts.append(results[count_key])
                rmse_values.append(results[f'{class_name}_rmse'])
                mae_values.append(results[f'{class_name}_mae'])
                r2_values.append(results[f'{class_name}_r2'])
            else:
                counts.append(0)
                rmse_values.append(0)
                mae_values.append(0)
                r2_values.append(0)
        
        # í”Œë¡¯ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ìƒ˜í”Œ ìˆ˜
        axes[0, 0].bar(class_labels, counts, color='skyblue', alpha=0.8)
        axes[0, 0].set_title('êµ¬ê°„ë³„ ìƒ˜í”Œ ìˆ˜')
        axes[0, 0].set_ylabel('ìƒ˜í”Œ ìˆ˜')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. RMSE
        axes[0, 1].bar(class_labels, rmse_values, color='lightcoral', alpha=0.8)
        axes[0, 1].set_title('êµ¬ê°„ë³„ RMSE')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. MAE
        axes[1, 0].bar(class_labels, mae_values, color='lightgreen', alpha=0.8)
        axes[1, 0].set_title('êµ¬ê°„ë³„ MAE')
        axes[1, 0].set_ylabel('MAE')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. RÂ²
        axes[1, 1].bar(class_labels, r2_values, color='gold', alpha=0.8)
        axes[1, 1].set_title('êµ¬ê°„ë³„ RÂ²')
        axes[1, 1].set_ylabel('RÂ²')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„ì„ ì €ì¥: {save_path}")
        
        plt.show()
    
    @staticmethod
    def create_evaluation_report(results: Dict[str, Any],
                               save_dir: str = "evaluation_results"):
        """ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True)
        
        y_true = results['targets']
        y_pred = results['predictions']
        dataset_name = results.get('dataset_name', 'Dataset')
        
        # 1. ì˜ˆì¸¡ vs ì‹¤ì œê°’ í”Œë¡¯
        VisualizationTools.plot_prediction_vs_actual(
            y_true, y_pred,
            title=f"{dataset_name} - ì˜ˆì¸¡ ì„±ëŠ¥",
            save_path=save_dir / f"{dataset_name}_prediction_vs_actual.png"
        )
        
        # 2. ì”ì°¨ ë¶„ì„
        VisualizationTools.plot_residual_analysis(
            y_true, y_pred,
            save_path=save_dir / f"{dataset_name}_residual_analysis.png"
        )
        
        # 3. êµ¬ê°„ë³„ ì„±ëŠ¥
        VisualizationTools.plot_class_performance(
            results,
            save_path=save_dir / f"{dataset_name}_class_performance.png"
        )
        
        # 4. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
        report_path = save_dir / f"{dataset_name}_evaluation_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸\n")
            f.write(f"{'='*50}\n")
            f.write(f"ë°ì´í„°ì…‹: {dataset_name}\n")
            f.write(f"ìƒ˜í”Œ ìˆ˜: {results['sample_count']}\n\n")
            
            f.write(f"ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­:\n")
            f.write(f"  - RMSE: {results['rmse']:.4f}\n")
            f.write(f"  - MAE: {results['mae']:.4f}\n")
            f.write(f"  - RÂ²: {results['r2']:.4f}\n")
            f.write(f"  - ì¡°ì •ëœ RÂ²: {results['adjusted_r2']:.4f}\n")
            f.write(f"  - MAPE: {results['mape']:.2f}%\n")
            f.write(f"  - NRMSE: {results['nrmse']:.2f}%\n\n")
            
            f.write(f"ë‹¹ë„ ë²”ìœ„ë³„ ì •í™•ë„:\n")
            for key, value in results.items():
                if key.startswith('accuracy_within'):
                    tolerance = key.split('_')[2]
                    f.write(f"  - Â±{tolerance} Brix ì´ë‚´: {value:.1f}%\n")
            
            f.write(f"\nêµ¬ê°„ë³„ ì„±ëŠ¥:\n")
            class_names = ['Low', 'Medium', 'High', 'Very_High']
            for class_name in class_names:
                count_key = f'{class_name}_count'
                if count_key in results and results[count_key] > 0:
                    rmse = results[f'{class_name}_rmse']
                    mae = results[f'{class_name}_mae']
                    r2 = results[f'{class_name}_r2']
                    count = results[count_key]
                    f.write(f"  - {class_name} êµ¬ê°„ (n={count}): RMSE={rmse:.3f}, MAE={mae:.3f}, RÂ²={r2:.3f}\n")
        
        print(f"ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {save_dir}")


def compare_models(model_results: Dict[str, Dict[str, Any]],
                  save_path: Optional[str] = None):
    """ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ"""
    
    model_names = list(model_results.keys())
    metrics = ['rmse', 'mae', 'r2', 'accuracy_within_0.5_brix', 'accuracy_within_1.0_brix']
    metric_labels = ['RMSE', 'MAE', 'RÂ²', 'Â±0.5 Brix ì •í™•ë„', 'Â±1.0 Brix ì •í™•ë„']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
        values = [model_results[name][metric] for name in model_names]
        
        bars = axes[i].bar(model_names, values, alpha=0.8)
        axes[i].set_title(f'ëª¨ë¸ë³„ {label} ë¹„êµ')
        axes[i].set_ylabel(label)
        axes[i].tick_params(axis='x', rotation=45)
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, value in zip(bars, values):
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{value:.3f}', ha='center', va='bottom')
    
    # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
    axes[5].set_visible(False)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì €ì¥: {save_path}")
    
    plt.show()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ
    print("Evaluation ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ì‚¬ìš© ì˜ˆì œ:")
    print("evaluator = ModelEvaluator(model)")
    print("results = evaluator.evaluate_comprehensive(test_loader)")
    print("VisualizationTools.create_evaluation_report(results)") 