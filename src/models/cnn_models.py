"""
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ì„ ìœ„í•œ CNN ëª¨ë¸ ì„¤ê³„

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. VGG-16 ê¸°ë°˜ ì „ì´í•™ìŠµ ëª¨ë¸ (íšŒê·€ìš©ìœ¼ë¡œ ìˆ˜ì •)
2. ìˆ˜ë°• ìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŠ¹í™” ì»¤ìŠ¤í…€ CNN ëª¨ë¸
3. ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
4. ëª¨ë¸ ì´ˆê¸°í™” ë° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

Author: AI Assistant  
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR
import torchvision.models as models
from typing import Dict, Any, Optional, Tuple
import math


class VGG16Regressor(nn.Module):
    """
    VGG-16 ê¸°ë°˜ íšŒê·€ ëª¨ë¸
    
    ì‚¬ì „í›ˆë ¨ëœ VGG-16 ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ì„ ìœ„í•œ íšŒê·€ ëª¨ë¸ë¡œ ìˆ˜ì •.
    ë¶„ë¥˜ê¸° ë¶€ë¶„ì„ íšŒê·€ìš©ìœ¼ë¡œ êµì²´í•˜ê³  Fine-tuning ì „ëµì„ ì ìš©.
    
    Features:
    - ì‚¬ì „í›ˆë ¨ëœ íŠ¹ì„± ì¶”ì¶œê¸° í™œìš©
    - ë°°ì¹˜ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ ì¶”ê°€
    - ìœ ì—°í•œ Fine-tuning ì „ëµ (íŠ¹ì„± ì¶”ì¶œê¸° ê³ ì •/í•´ì œ)
    """
    
    def __init__(self, 
                 num_classes: int = 1,
                 dropout: float = 0.5,
                 freeze_features: bool = False,
                 use_batch_norm: bool = True,
                 use_pretrained: bool = True):
        """
        Args:
            num_classes (int): ì¶œë ¥ ì°¨ì› (íšŒê·€ì˜ ê²½ìš° 1)
            dropout (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            freeze_features (bool): íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ê³ ì •í• ì§€ ì—¬ë¶€
            use_batch_norm (bool): ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
            use_pretrained (bool): ì‚¬ì „í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
        """
        super(VGG16Regressor, self).__init__()
        
        # VGG-16 ë¡œë”© (ì‚¬ì „í›ˆë ¨ ì—¬ë¶€ ì„ íƒ ê°€ëŠ¥)
        try:
            if use_pretrained:
                self.vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)
            else:
                self.vgg16 = models.vgg16(weights=None)
        except Exception as e:
            print(f"ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ì‚¬ì „í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.vgg16 = models.vgg16(weights=None)
        
        # íŠ¹ì„± ì¶”ì¶œê¸° ê³ ì • ì„¤ì •
        if freeze_features:
            for param in self.vgg16.features.parameters():
                param.requires_grad = False
        
        # ë¶„ë¥˜ê¸°ë¥¼ íšŒê·€ìš©ìœ¼ë¡œ êµì²´
        # VGG-16ì˜ ì›ë˜ ë¶„ë¥˜ê¸°: Linear(25088, 4096) -> ReLU -> Dropout -> Linear(4096, 4096) -> ReLU -> Dropout -> Linear(4096, 1000)
        
        classifier_layers = []
        
        # ì²« ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ
        classifier_layers.append(nn.Linear(25088, 4096))
        if use_batch_norm:
            classifier_layers.append(nn.BatchNorm1d(4096))
        classifier_layers.append(nn.ReLU(inplace=True))
        classifier_layers.append(nn.Dropout(dropout))
        
        # ë‘ ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ
        classifier_layers.append(nn.Linear(4096, 1024))
        if use_batch_norm:
            classifier_layers.append(nn.BatchNorm1d(1024))
        classifier_layers.append(nn.ReLU(inplace=True))
        classifier_layers.append(nn.Dropout(dropout))
        
        # ì„¸ ë²ˆì§¸ ì™„ì „ì—°ê²°ì¸µ (ì¤‘ê°„ ì°¨ì›)
        classifier_layers.append(nn.Linear(1024, 256))
        if use_batch_norm:
            classifier_layers.append(nn.BatchNorm1d(256))
        classifier_layers.append(nn.ReLU(inplace=True))
        classifier_layers.append(nn.Dropout(dropout / 2))  # ë§ˆì§€ë§‰ ë“œë¡­ì•„ì›ƒì€ ì ˆë°˜ìœ¼ë¡œ
        
        # ì¶œë ¥ì¸µ (íšŒê·€)
        classifier_layers.append(nn.Linear(256, num_classes))
        
        # ê¸°ì¡´ ë¶„ë¥˜ê¸° êµì²´
        self.vgg16.classifier = nn.Sequential(*classifier_layers)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ë¥¼ He ì´ˆê¸°í™”ë¡œ ì„¤ì •"""
        for m in self.vgg16.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ìˆœì „íŒŒ
        
        Args:
            x (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, C, H, W]
            
        Returns:
            torch.Tensor: ì˜ˆì¸¡ëœ Brix ê°’ [B, 1]
        """
        return self.vgg16(x)
    
    def unfreeze_features(self, unfreeze_from_layer: int = -1):
        """
        íŠ¹ì„± ì¶”ì¶œê¸°ì˜ ì¼ë¶€ ë˜ëŠ” ì „ì²´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        
        Args:
            unfreeze_from_layer (int): í•´ì œí•  ì‹œì‘ ë ˆì´ì–´ (-1ì´ë©´ ì „ì²´ í•´ì œ)
        """
        layers = list(self.vgg16.features.children())
        
        if unfreeze_from_layer == -1:
            # ì „ì²´ í•´ì œ
            for param in self.vgg16.features.parameters():
                param.requires_grad = True
        else:
            # íŠ¹ì • ë ˆì´ì–´ë¶€í„° í•´ì œ
            for i, layer in enumerate(layers):
                if i >= unfreeze_from_layer:
                    for param in layer.parameters():
                        param.requires_grad = True


class WatermelonCNN(nn.Module):
    """
    ìˆ˜ë°• ìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŠ¹í™” ì»¤ìŠ¤í…€ CNN ëª¨ë¸
    
    ìˆ˜ë°• íƒ€ê²©ìŒ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜.
    ì”ì°¨ ì—°ê²°ê³¼ ê¸€ë¡œë²Œ í‰ê·  í’€ë§ì„ í™œìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€.
    
    Features:
    - ì ì§„ì  ì±„ë„ í™•ì¥ (32 -> 64 -> 128 -> 256)
    - ì”ì°¨ ì—°ê²° (ResNet ìŠ¤íƒ€ì¼)
    - ê¸€ë¡œë²Œ í‰ê·  í’€ë§ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ ê°ì†Œ
    - ë°°ì¹˜ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
    """
    
    def __init__(self, 
                 input_channels: int = 3,
                 num_classes: int = 1,
                 dropout: float = 0.3,
                 use_residual: bool = True):
        """
        Args:
            input_channels (int): ì…ë ¥ ì±„ë„ ìˆ˜ (RGB=3)
            num_classes (int): ì¶œë ¥ ì°¨ì› (íšŒê·€ì˜ ê²½ìš° 1)
            dropout (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            use_residual (bool): ì”ì°¨ ì—°ê²° ì‚¬ìš© ì—¬ë¶€
        """
        super(WatermelonCNN, self).__init__()
        
        self.use_residual = use_residual
        
        # ì²« ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡ (3 -> 32)
        self.conv1 = self._make_conv_block(input_channels, 32, kernel_size=7, stride=2, padding=3)
        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # ë‘ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡ (32 -> 64)
        self.conv2_1 = self._make_conv_block(32, 64, kernel_size=3, stride=1, padding=1)
        self.conv2_2 = self._make_conv_block(64, 64, kernel_size=3, stride=1, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ì„¸ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡ (64 -> 128) with residual
        self.conv3_1 = self._make_conv_block(64, 128, kernel_size=3, stride=2, padding=1)
        self.conv3_2 = self._make_conv_block(128, 128, kernel_size=3, stride=1, padding=1)
        
        # ë„¤ ë²ˆì§¸ í•©ì„±ê³± ë¸”ë¡ (128 -> 256) with residual
        self.conv4_1 = self._make_conv_block(128, 256, kernel_size=3, stride=2, padding=1)
        self.conv4_2 = self._make_conv_block(256, 256, kernel_size=3, stride=1, padding=1)
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ 1x1 í•©ì„±ê³±
        if use_residual:
            self.shortcut3 = nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False)
            self.shortcut4 = nn.Conv2d(128, 256, kernel_size=1, stride=2, bias=False)
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # ì™„ì „ì—°ê²° ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout / 2),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout / 4),
            nn.Linear(64, num_classes)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
    
    def _make_conv_block(self, in_channels: int, out_channels: int, 
                        kernel_size: int = 3, stride: int = 1, padding: int = 1) -> nn.Sequential:
        """í•©ì„±ê³± ë¸”ë¡ ìƒì„± (Conv -> BatchNorm -> ReLU)"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (He ì´ˆê¸°í™”)"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ìˆœì „íŒŒ
        
        Args:
            x (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, C, H, W]
            
        Returns:
            torch.Tensor: ì˜ˆì¸¡ëœ Brix ê°’ [B, 1]
        """
        # ì²« ë²ˆì§¸ ë¸”ë¡
        x = self.conv1(x)
        x = self.pool1(x)
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡
        x = self.conv2_1(x)
        x = self.conv2_2(x)
        x = self.pool2(x)
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡ (ì”ì°¨ ì—°ê²°)
        if self.use_residual:
            identity = self.shortcut3(x)
            x = self.conv3_1(x)
            x = self.conv3_2(x)
            x = x + identity
        else:
            x = self.conv3_1(x)
            x = self.conv3_2(x)
        
        # ë„¤ ë²ˆì§¸ ë¸”ë¡ (ì”ì°¨ ì—°ê²°)
        if self.use_residual:
            identity = self.shortcut4(x)
            x = self.conv4_1(x)
            x = self.conv4_2(x)
            x = x + identity
        else:
            x = self.conv4_1(x)
            x = self.conv4_2(x)
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        x = self.global_avg_pool(x)
        x = x.view(x.size(0), -1)  # Flatten
        
        # ë¶„ë¥˜ê¸°
        x = self.classifier(x)
        
        return x


class ModelFactory:
    """ëª¨ë¸ ìƒì„±ì„ ìœ„í•œ íŒ©í† ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def create_model(model_type: str, **kwargs) -> nn.Module:
        """
        ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ëª¨ë¸ ìƒì„±
        
        Args:
            model_type (str): ëª¨ë¸ íƒ€ì… ('vgg16', 'custom_cnn')
            **kwargs: ëª¨ë¸ë³„ ì¶”ê°€ ì¸ì
            
        Returns:
            nn.Module: ìƒì„±ëœ ëª¨ë¸
        """
        if model_type.lower() == 'vgg16':
            return VGG16Regressor(**kwargs)
        elif model_type.lower() == 'custom_cnn':
            return WatermelonCNN(**kwargs)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")


class LossManager:
    """
    ì†ì‹¤í•¨ìˆ˜ ê´€ë¦¬ í´ë˜ìŠ¤
    
    íšŒê·€ ë¬¸ì œì— ì í•©í•œ ë‹¤ì–‘í•œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì œê³µí•˜ê³ 
    í•„ìš”ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë‚˜ ì •ê·œí™”ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    @staticmethod
    def get_loss_function(loss_type: str, **kwargs) -> nn.Module:
        """
        ì†ì‹¤í•¨ìˆ˜ ìƒì„±
        
        Args:
            loss_type (str): ì†ì‹¤í•¨ìˆ˜ íƒ€ì…
            **kwargs: ì†ì‹¤í•¨ìˆ˜ë³„ ì¶”ê°€ ì¸ì
            
        Returns:
            nn.Module: ì†ì‹¤í•¨ìˆ˜
        """
        if loss_type.lower() == 'mse':
            return nn.MSELoss(**kwargs)
        elif loss_type.lower() == 'mae' or loss_type.lower() == 'l1':
            return nn.L1Loss(**kwargs)
        elif loss_type.lower() == 'huber' or loss_type.lower() == 'smooth_l1':
            return nn.SmoothL1Loss(**kwargs)
        elif loss_type.lower() == 'rmse':
            return RMSELoss(**kwargs)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì†ì‹¤í•¨ìˆ˜: {loss_type}")


class RMSELoss(nn.Module):
    """Root Mean Square Error ì†ì‹¤í•¨ìˆ˜"""
    
    def __init__(self, eps: float = 1e-8):
        super(RMSELoss, self).__init__()
        self.eps = eps
        
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        return torch.sqrt(F.mse_loss(pred, target) + self.eps)


class OptimizerManager:
    """
    ì˜µí‹°ë§ˆì´ì € ê´€ë¦¬ í´ë˜ìŠ¤
    
    ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    @staticmethod
    def get_optimizer(model: nn.Module, optimizer_type: str, **kwargs) -> optim.Optimizer:
        """
        ì˜µí‹°ë§ˆì´ì € ìƒì„±
        
        Args:
            model (nn.Module): ëª¨ë¸
            optimizer_type (str): ì˜µí‹°ë§ˆì´ì € íƒ€ì…
            **kwargs: ì˜µí‹°ë§ˆì´ì €ë³„ ì¶”ê°€ ì¸ì
            
        Returns:
            optim.Optimizer: ì˜µí‹°ë§ˆì´ì €
        """
        if optimizer_type.lower() == 'adam':
            return optim.Adam(model.parameters(), **kwargs)
        elif optimizer_type.lower() == 'adamw':
            return optim.AdamW(model.parameters(), **kwargs)
        elif optimizer_type.lower() == 'sgd':
            return optim.SGD(model.parameters(), **kwargs)
        elif optimizer_type.lower() == 'rmsprop':
            return optim.RMSprop(model.parameters(), **kwargs)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_type}")
    
    @staticmethod
    def get_scheduler(optimizer: optim.Optimizer, scheduler_type: str, **kwargs):
        """
        í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
        
        Args:
            optimizer (optim.Optimizer): ì˜µí‹°ë§ˆì´ì €
            scheduler_type (str): ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
            **kwargs: ìŠ¤ì¼€ì¤„ëŸ¬ë³„ ì¶”ê°€ ì¸ì
            
        Returns:
            í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        """
        if scheduler_type.lower() == 'step':
            return StepLR(optimizer, **kwargs)
        elif scheduler_type.lower() == 'plateau':
            return ReduceLROnPlateau(optimizer, **kwargs)
        elif scheduler_type.lower() == 'cosine':
            return CosineAnnealingLR(optimizer, **kwargs)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ìŠ¤ì¼€ì¤„ëŸ¬: {scheduler_type}")


def get_model_info(model: nn.Module) -> Dict[str, Any]:
    """
    ëª¨ë¸ ì •ë³´ ì¶œë ¥
    
    Args:
        model (nn.Module): ëª¨ë¸
        
    Returns:
        Dict[str, Any]: ëª¨ë¸ ì •ë³´
    """
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        'model_type': model.__class__.__name__,
        'total_parameters': total_params,
        'trainable_parameters': trainable_params,
        'frozen_parameters': total_params - trainable_params,
        'model_size_mb': total_params * 4 / (1024 * 1024)  # 4 bytes per float32
    }


def print_model_summary(model: nn.Module, input_size: Tuple[int, ...] = (3, 224, 224)):
    """
    ëª¨ë¸ ìš”ì•½ ì •ë³´ ì¶œë ¥
    
    Args:
        model (nn.Module): ëª¨ë¸
        input_size (Tuple[int, ...]): ì…ë ¥ í¬ê¸° (C, H, W)
    """
    info = get_model_info(model)
    
    print(f"\n{'='*50}")
    print(f"ëª¨ë¸ ìš”ì•½: {info['model_type']}")
    print(f"{'='*50}")
    print(f"ì´ ë§¤ê°œë³€ìˆ˜ ìˆ˜: {info['total_parameters']:,}")
    print(f"í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜: {info['trainable_parameters']:,}")
    print(f"ê³ ì •ëœ ë§¤ê°œë³€ìˆ˜: {info['frozen_parameters']:,}")
    print(f"ëª¨ë¸ í¬ê¸°: {info['model_size_mb']:.2f} MB")
    
    # ìƒ˜í”Œ ì…ë ¥ìœ¼ë¡œ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    try:
        model.eval()
        with torch.no_grad():
            sample_input = torch.randn(1, *input_size)
            output = model(sample_input)
            print(f"ì…ë ¥ í¬ê¸°: {list(sample_input.shape)}")
            print(f"ì¶œë ¥ í¬ê¸°: {list(output.shape)}")
    except Exception as e:
        print(f"ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"{'='*50}\n")


if __name__ == "__main__":
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì˜ˆì œ
    print("ğŸ”§ ëª¨ë¸ ì„¤ê³„ í…ŒìŠ¤íŠ¸")
    
    # VGG-16 ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n1. VGG-16 ê¸°ë°˜ íšŒê·€ ëª¨ë¸")
    vgg_model = ModelFactory.create_model('vgg16', dropout=0.5, freeze_features=False, use_pretrained=False)
    print_model_summary(vgg_model)
    
    # ì»¤ìŠ¤í…€ CNN ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n2. ì»¤ìŠ¤í…€ CNN ëª¨ë¸")
    custom_model = ModelFactory.create_model('custom_cnn', dropout=0.3, use_residual=True)
    print_model_summary(custom_model)
    
    # ì†ì‹¤í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    print("\n3. ì†ì‹¤í•¨ìˆ˜ í…ŒìŠ¤íŠ¸")
    loss_fns = ['mse', 'mae', 'huber', 'rmse']
    for loss_type in loss_fns:
        loss_fn = LossManager.get_loss_function(loss_type)
        print(f"  - {loss_type.upper()}: {loss_fn.__class__.__name__}")
    
    # ì˜µí‹°ë§ˆì´ì € í…ŒìŠ¤íŠ¸
    print("\n4. ì˜µí‹°ë§ˆì´ì € í…ŒìŠ¤íŠ¸")
    optimizers = ['adam', 'adamw', 'sgd']
    for opt_type in optimizers:
        optimizer = OptimizerManager.get_optimizer(custom_model, opt_type, lr=0.001)
        print(f"  - {opt_type.upper()}: {optimizer.__class__.__name__}")
    
    print("\nâœ… ëª¨ë¸ ì„¤ê³„ ì™„ë£Œ!") 