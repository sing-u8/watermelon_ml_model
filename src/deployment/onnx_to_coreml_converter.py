"""
ONNX ëª¨ë¸ì„ Core MLë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì´ ëª¨ë“ˆì€ ONNX í˜•ì‹ì˜ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì„ Apple Core ML í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
iOS/macOS ì•±ì—ì„œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”ë©ë‹ˆë‹¤.

Author: AI Assistant
Date: 2024
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import numpy as np

# Core ML ë³€í™˜ ë„êµ¬
import coremltools as ct
from coremltools.models import MLModel

# ONNX ê´€ë ¨
import onnx
from onnx import numpy_helper

# ì´ë¯¸ì§€ ì²˜ë¦¬
from PIL import Image
import cv2


class ONNXToCoreMLConverter:
    """ONNX ëª¨ë¸ì„ Core MLë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 onnx_model_path: str,
                 output_dir: str = "models/coreml",
                 model_name: str = "WatermelonBrixPredictor"):
        """
        Args:
            onnx_model_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            output_dir: Core ML ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬  
            model_name: Core ML ëª¨ë¸ ì´ë¦„
        """
        self.onnx_model_path = Path(onnx_model_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.model_name = model_name
        
        # ëª¨ë¸ ì •ë³´ ë¡œë“œ
        self.model_info = self._load_model_info()
        
        print(f"ONNX ëª¨ë¸ ê²½ë¡œ: {self.onnx_model_path}")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"ëª¨ë¸ ì´ë¦„: {self.model_name}")
        
    def _load_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ JSON íŒŒì¼ ë¡œë“œ"""
        info_path = self.onnx_model_path.parent / f"{self.onnx_model_path.stem}_info.json"
        
        if info_path.exists():
            with open(info_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            print(f"ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {info_path}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "input_shape": [1, 3, 224, 224],
                "input_name": "melspectrogram_image",
                "output_name": "brix_prediction",
                "preprocessing": {
                    "normalize": {
                        "mean": [0.485, 0.456, 0.406],
                        "std": [0.229, 0.224, 0.225]
                    }
                }
            }
    
    def _verify_onnx_model(self) -> onnx.ModelProto:
        """ONNX ëª¨ë¸ ê²€ì¦ ë° ë¡œë“œ"""
        print("\nONNX ëª¨ë¸ ê²€ì¦ ì¤‘...")
        
        if not self.onnx_model_path.exists():
            raise FileNotFoundError(f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.onnx_model_path}")
        
        # ONNX ëª¨ë¸ ë¡œë“œ
        onnx_model = onnx.load(str(self.onnx_model_path))
        
        # ëª¨ë¸ ê²€ì¦
        try:
            onnx.checker.check_model(onnx_model)
            print("âœ… ONNX ëª¨ë¸ ê²€ì¦ í†µê³¼")
        except Exception as e:
            print(f"âš ï¸ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            print("ë³€í™˜ì„ ê³„ì† ì‹œë„í•©ë‹ˆë‹¤...")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        print(f"ONNX ëª¨ë¸ ì •ë³´:")
        print(f"  - IR ë²„ì „: {onnx_model.ir_version}")
        print(f"  - í”„ë¡œë“€ì„œ: {onnx_model.producer_name} {onnx_model.producer_version}")
        
        # ì…ë ¥ ì •ë³´
        for inp in onnx_model.graph.input:
            shape = [dim.dim_value for dim in inp.type.tensor_type.shape.dim]
            print(f"  - ì…ë ¥: {inp.name}, í˜•íƒœ: {shape}")
        
        # ì¶œë ¥ ì •ë³´  
        for out in onnx_model.graph.output:
            shape = [dim.dim_value for dim in out.type.tensor_type.shape.dim]
            print(f"  - ì¶œë ¥: {out.name}, í˜•íƒœ: {shape}")
        
        return onnx_model
    
    def convert_to_coreml(self, 
                         compute_precision: str = "FLOAT16",
                         minimum_deployment_target: str = "iOS13") -> MLModel:
        """ONNX ëª¨ë¸ì„ Core MLë¡œ ë³€í™˜"""
        print("\n" + "="*60)
        print("ONNX â†’ Core ML ë³€í™˜ ì‹œì‘")
        print("="*60)
        
        # ONNX ëª¨ë¸ ê²€ì¦
        onnx_model = self._verify_onnx_model()
        
        # Core ML ë³€í™˜
        print("\nCore ML ë³€í™˜ ì§„í–‰ ì¤‘...")
        
        try:
            # ë°°í¬ íƒ€ê²Ÿ ì„¤ì •
            if minimum_deployment_target == "iOS13":
                target = ct.target.iOS13
            elif minimum_deployment_target == "iOS14":
                target = ct.target.iOS14
            elif minimum_deployment_target == "iOS15": 
                target = ct.target.iOS15
            elif minimum_deployment_target == "iOS16":
                target = ct.target.iOS16
            else:
                target = ct.target.iOS13
            
            # ONNXì—ì„œ Core MLë¡œ ë³€í™˜
            coreml_model = ct.convert(
                str(self.onnx_model_path),
                minimum_deployment_target=target,
                compute_precision=ct.precision.FLOAT16 if compute_precision == "FLOAT16" else ct.precision.FLOAT32
            )
            
            print(f"âœ… Core ML ë³€í™˜ ì„±ê³µ!")
            
        except Exception as e:
            print(f"âŒ Core ML ë³€í™˜ ì‹¤íŒ¨: {e}")
            print("ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„ ì¤‘...")
            
            # ëŒ€ì•ˆ ë°©ë²•: ë” ê¸°ë³¸ì ì¸ ì„¤ì •ìœ¼ë¡œ ì‹œë„
            try:
                coreml_model = ct.convert(str(self.onnx_model_path))
                print(f"âœ… ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ Core ML ë³€í™˜ ì„±ê³µ!")
            except Exception as e2:
                raise Exception(f"ëª¨ë“  ë³€í™˜ ë°©ë²• ì‹¤íŒ¨: {e2}")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        self._add_metadata(coreml_model)
        
        # ëª¨ë¸ ê²€ì¦
        self._validate_coreml_model(coreml_model)
        
        return coreml_model
    
    def _add_metadata(self, coreml_model: MLModel):
        """Core ML ëª¨ë¸ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        print("\në©”íƒ€ë°ì´í„° ì¶”ê°€ ì¤‘...")
        
        # ê¸°ë³¸ ì •ë³´
        coreml_model.short_description = "ìˆ˜ë°• ë‹¹ë„(Brix) ì˜ˆì¸¡ AI ëª¨ë¸ - ONNX ë³€í™˜"
        coreml_model.author = "WatermelonAI Team"
        coreml_model.license = "MIT License"
        coreml_model.version = "1.0.0"
        
        # ì…ë ¥/ì¶œë ¥ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        input_name = None
        output_name = None
        
        for inp in coreml_model.get_spec().description.input:
            input_name = inp.name
            break
            
        for out in coreml_model.get_spec().description.output:
            output_name = out.name
            break
        
        # ì…ë ¥ ì„¤ëª…
        if input_name:
            coreml_model.input_description[input_name] = (
                "ìˆ˜ë°• íƒ€ê²©ìŒì—ì„œ ì¶”ì¶œí•œ ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ (224x224 RGB). "
                "ImageNet í‘œì¤€ìœ¼ë¡œ ì •ê·œí™”ëœ ìƒíƒœì—¬ì•¼ í•¨."
            )
        
        # ì¶œë ¥ ì„¤ëª…
        if output_name:
            coreml_model.output_description[output_name] = (
                "ì˜ˆì¸¡ëœ ìˆ˜ë°• ë‹¹ë„ ê°’ (Brix). "
                "ì¼ë°˜ì ìœ¼ë¡œ 8.0-13.0 ë²”ìœ„ì˜ ê°’ì„ ê°€ì§."
            )
        
        # ì‚¬ìš©ë²• ë©”íƒ€ë°ì´í„°
        coreml_model.user_defined_metadata["preprocessing_info"] = (
            "1. ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 224x224ë¡œ ì¡°ì • "
            "2. RGB ì±„ë„ë¡œ ë³€í™˜ "
            "3. [0,1]ë¡œ ì •ê·œí™” í›„ ImageNet í‰ê· /í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”: "
            f"mean={self.model_info['preprocessing']['normalize']['mean']}, "
            f"std={self.model_info['preprocessing']['normalize']['std']}"
        )
        
        coreml_model.user_defined_metadata["model_source"] = "ONNX ëª¨ë¸ì—ì„œ ë³€í™˜ë¨"
        coreml_model.user_defined_metadata["original_model"] = str(self.onnx_model_path)
        
        # ì„±ëŠ¥ ì •ë³´ (JSONì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        if "performance" in self.model_info:
            perf = self.model_info["performance"]
            coreml_model.user_defined_metadata["performance"] = (
                f"RMSE: {perf.get('rmse', 'N/A')}, "
                f"ë‹¹ë„ ì •í™•ë„(Â±1.0 Brix): {perf.get('accuracy_1_brix', 'N/A')}%"
            )
        
        print("âœ… ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
    
    def _validate_coreml_model(self, coreml_model: MLModel):
        """Core ML ëª¨ë¸ ê²€ì¦"""
        print("\nCore ML ëª¨ë¸ ê²€ì¦ ì¤‘...")
        
        try:
            # ëª¨ë¸ ìŠ¤í™ ê°€ì ¸ì˜¤ê¸°
            spec = coreml_model.get_spec()
            
            print(f"ëª¨ë¸ ìœ í˜•: {spec.WhichOneof('Type')}")
            print(f"ì…ë ¥ ê°œìˆ˜: {len(spec.description.input)}")
            print(f"ì¶œë ¥ ê°œìˆ˜: {len(spec.description.output)}")
            
            # ì…ë ¥ ì •ë³´ ì¶œë ¥
            for inp in spec.description.input:
                print(f"  ì…ë ¥: {inp.name}")
                if inp.type.WhichOneof('Type') == 'multiArrayType':
                    print(f"    - íƒ€ì…: MultiArray")
                    print(f"    - í˜•íƒœ: {list(inp.type.multiArrayType.shape)}")
                    print(f"    - ë°ì´í„° íƒ€ì…: {inp.type.multiArrayType.dataType}")
                elif inp.type.WhichOneof('Type') == 'imageType':
                    print(f"    - íƒ€ì…: Image")
                    print(f"    - í­: {inp.type.imageType.width}")
                    print(f"    - ë†’ì´: {inp.type.imageType.height}")
            
            # ì¶œë ¥ ì •ë³´ ì¶œë ¥
            for out in spec.description.output:
                print(f"  ì¶œë ¥: {out.name}")
                if out.type.WhichOneof('Type') == 'multiArrayType':
                    print(f"    - íƒ€ì…: MultiArray")
                    print(f"    - í˜•íƒœ: {list(out.type.multiArrayType.shape)}")
                    print(f"    - ë°ì´í„° íƒ€ì…: {out.type.multiArrayType.dataType}")
            
            print("âœ… Core ML ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ Core ML ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_model(self, coreml_model: MLModel, filename: Optional[str] = None) -> str:
        """Core ML ëª¨ë¸ ì €ì¥"""
        if filename is None:
            filename = f"{self.model_name}.mlmodel"
        
        output_path = self.output_dir / filename
        
        print(f"\nCore ML ëª¨ë¸ ì €ì¥ ì¤‘: {output_path}")
        
        try:
            coreml_model.save(str(output_path))
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = output_path.stat().st_size / (1024 * 1024)  # MB
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
            print(f"   íŒŒì¼ ê²½ë¡œ: {output_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
            
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def test_model_inference(self, coreml_model: MLModel, num_tests: int = 3):
        """Core ML ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
        print(f"\nCore ML ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ({num_tests}íšŒ)...")
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
        input_shape = self.model_info["input_shape"]
        test_input = np.random.randn(*input_shape).astype(np.float32)
        
        # ImageNet ì •ê·œí™” ì ìš©
        mean = np.array(self.model_info["preprocessing"]["normalize"]["mean"])
        std = np.array(self.model_info["preprocessing"]["normalize"]["std"])
        
        # ì •ê·œí™” (C, H, W í˜•ì‹)
        for c in range(3):
            test_input[0, c] = (test_input[0, c] - mean[c]) / std[c]
        
        inference_times = []
        predictions = []
        
        try:
            # ì…ë ¥ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            input_name = None
            for inp in coreml_model.get_spec().description.input:
                input_name = inp.name
                break
            
            if input_name is None:
                print("âŒ ì…ë ¥ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            for i in range(num_tests):
                start_time = time.time()
                
                # Core ML ì˜ˆì¸¡
                prediction = coreml_model.predict({input_name: test_input})
                
                inference_time = (time.time() - start_time) * 1000  # ms
                inference_times.append(inference_time)
                
                # ì˜ˆì¸¡ê°’ ì¶”ì¶œ
                pred_value = list(prediction.values())[0]
                if isinstance(pred_value, np.ndarray):
                    pred_value = pred_value.flatten()[0]
                predictions.append(pred_value)
                
                print(f"  í…ŒìŠ¤íŠ¸ {i+1}: {pred_value:.4f} Brix, {inference_time:.2f}ms")
            
            # í†µê³„ ì¶œë ¥
            avg_time = np.mean(inference_times)
            avg_pred = np.mean(predictions)
            std_pred = np.std(predictions)
            
            print(f"\nğŸ“Š ì¶”ë¡  ì„±ëŠ¥ í†µê³„:")
            print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}ms")
            print(f"   í‰ê·  ì˜ˆì¸¡ê°’: {avg_pred:.4f} Brix")
            print(f"   ì˜ˆì¸¡ê°’ í‘œì¤€í¸ì°¨: {std_pred:.4f}")
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ‰ ONNX â†’ Core ML ë³€í™˜ ë„êµ¬")
    print("="*50)
    
    # ONNX ëª¨ë¸ ê²½ë¡œ
    onnx_model_path = "models/onnx/WatermelonBrixPredictor.onnx"
    
    # ë³€í™˜ê¸° ìƒì„±
    converter = ONNXToCoreMLConverter(
        onnx_model_path=onnx_model_path,
        output_dir="models/coreml",
        model_name="WatermelonBrixPredictor"
    )
    
    try:
        # Core ML ë³€í™˜
        coreml_model = converter.convert_to_coreml(
            compute_precision="FLOAT16",
            minimum_deployment_target="iOS13"
        )
        
        # ëª¨ë¸ ì €ì¥
        output_path = converter.save_model(coreml_model)
        
        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
        converter.test_model_inference(coreml_model)
        
        print(f"\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
        print(f"Core ML ëª¨ë¸: {output_path}")
        print(f"\nğŸ“± iOS/macOS ì•±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"\nâŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 